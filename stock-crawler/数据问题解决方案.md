# 股票数据问题分析与解决方案

## 问题分析

根据你提供的数据库截图，发现以下问题：

### 1. 数据重复问题
- **stock_data表有11,292条数据**，但stock_info只有5,931只股票
- 说明存在重复的交易数据记录
- 原因：爬虫多次运行时没有正确去重

### 2. 数据不一致问题
- stock_info（股票基本信息）：5,931条
- stock_data（交易数据）：11,292条
- 理论上每只股票每天应该只有一条交易记录

### 3. F10、财务、分红数据不足
- **stock_f10**：101条（应该接近5,931条）
- **stock_financial**：301条（应该更多）
- **stock_dividend**：184条（正常，不是所有股票都有分红）
- 原因：爬取限制、接口限制、错误处理不当

### 4. 历史数据表为空
- stock_price_history：0条
- stock_recommendations：0条
- stocks：0条

## 解决方案

### 步骤1：数据清理和分析

```bash
# 进入爬虫目录
cd stock-crawler

# 运行数据清理脚本
python data_cleanup.py
```

这个脚本会：
- 分析重复数据情况
- 清理重复的交易记录
- 删除孤立数据（在stock_data中但不在stock_info中的记录）
- 创建必要的唯一索引防止未来重复
- 提供详细的数据统计信息

### 步骤2：补充缺失数据

```bash
# 运行增强爬虫，补充缺失的F10、财务、分红数据
python enhanced_crawler.py
```

这个脚本会：
- 智能识别缺失F10数据的股票并补充
- 爬取缺失的财务数据（按市值排序，优先大盘股）
- 补充分红配股数据
- 使用多重试机制和随机延迟避免被限制

### 步骤3：修复原有爬虫

对原有的`crawler.py`进行以下改进：

1. **添加更好的去重机制**
2. **改进错误处理**
3. **增加数据验证**

## 数据库索引优化

为了防止重复数据，需要添加以下索引：

```sql
-- 为stock_data表添加唯一索引
ALTER TABLE stock_data ADD UNIQUE INDEX unique_symbol_date (symbol, date);

-- 为stock_info表添加唯一索引（如果没有的话）
ALTER TABLE stock_info ADD UNIQUE INDEX unique_symbol (symbol);

-- 为stock_f10表添加唯一索引
ALTER TABLE stock_f10 ADD UNIQUE INDEX unique_symbol (symbol);
```

## 预期结果

清理和补充后，数据应该是：

- **stock_info**: ~5,931条（不变）
- **stock_data**: ~5,931条（去除重复后）
- **stock_f10**: ~5,000+条（大幅增加）
- **stock_financial**: ~1,000+条（显著增加）
- **stock_dividend**: ~500+条（适度增加）

## 使用建议

### 1. 定期清理
```bash
# 每周运行一次数据清理
python data_cleanup.py
```

### 2. 增量更新
```bash
# 只爬取基本数据（日常使用）
python main.py --mode basic

# 补充详细数据（周末运行）
python enhanced_crawler.py
```

### 3. 监控数据质量
- 定期检查数据统计
- 监控重复数据
- 验证数据完整性

## 注意事项

1. **接口限制**：财务数据接口有较严格的频率限制，建议：
   - 增加请求间隔（2-3秒）
   - 使用随机延迟
   - 分批处理，避免一次性请求过多

2. **数据源稳定性**：
   - akshare接口可能会变化
   - 建议定期测试和更新代码
   - 考虑使用多个数据源备份

3. **存储优化**：
   - 定期清理过期数据
   - 考虑数据压缩和归档
   - 监控数据库大小

## 故障排除

### 如果数据库连接失败
1. 检查`.env`文件中的数据库配置
2. 确认MySQL服务是否启动
3. 验证用户名密码是否正确

### 如果爬取失败率高
1. 增加延迟时间
2. 检查网络连接
3. 更新akshare版本：`pip install akshare --upgrade`

### 如果数据不一致
1. 运行数据清理脚本
2. 检查数据库索引
3. 验证爬虫逻辑